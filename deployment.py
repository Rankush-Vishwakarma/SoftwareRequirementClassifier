# -*- coding: utf-8 -*-
"""deployment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NNgvSfLn28DXyzVFJ2SH37MTUpU8Jwev
"""

import tensorflow as tf
import time
import pandas as pd
import numpy as np
import nltk.corpus
import string
import unicodedata
from nltk.corpus import stopwords
import spacy
import re
import requests
import pickle
import contractions
from textblob import TextBlob

def cleanText_(text):  
  """
    0. convert abbr. to original text i am using https://www.webopedia.com/reference/text-abbreviations/
    1. normalize the text.
    2. Removing Unicode Characters
    3. Removing Stopwords
    4. Stemming and Lemmatization
    5. remove puctuations.
    6. removing special and accented charcters
    7. removing numbers
  """

  gotAbbribiation = False

  nltk.download('stopwords',quiet=True)
  nlp_lemm = spacy.load('en_core_web_sm')

  # removing emails and replacing it with its username
  text = ' '.join([i.split('@')[0] if '@' in i else i for i in text.split()])
  # removing the numbers
  text = "".join([i for i in text if not re.search('\d',i)])
  #text = "".join(newText)

  # normalization to lower case
  text = text.lower()

  # handling the contraction in text 
  text = contractions.fix(text)

  # removing https/https links charcters
  text = re.sub(r"http\S+", "", text)

  # removing stopwords
  text = " ".join([word for word in text.split() if word not in (stopwords.words('english'))])

  # lemmetizing 
  text = nlp_lemm(text)
  text = " ".join([word.lemma_ if word.lemma_ != '-pron-' else word.text for word in text])

  # removing the puctuations
  text = "".join([ch for ch in text if ch not in string.punctuation])

  # removing the accented characters 
  text = unicodedata.normalize('NFKD',text).encode('ascii','ignore').decode('utf-8','ignore')

  # changing the abbribiations
  if gotAbbribiation:
    text = " ".join([abbrDict[i] if i in abbrDict.keys() else i for i in  text.split()])
  # removing \n, \t, \t\t from text
  text = re.sub(r'\s','',text)
  return str(text)

def load_Model_Tokenizer():
  model = tf.keras.models.load_model(r'risk_model_all')
  with open('tokenizer.pkl','rb') as token:
    tokenizer = pickle.load(token)
  return model , tokenizer


def prepare(text, tokenizer):
  tokens = tokenizer.encode_plus(text, max_length = 512, truncation = True,
                                 padding = 'max_length', add_special_tokens = True, 
                                  return_tensors = 'tf' )
  return {
      'input_ids': tf.cast(tokens['input_ids'], tf.float64),
      'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)
  }

def predict(text,model, tokenizer): 
  precessed_text = cleanText_(text)
  Prepared_text = prepare(precessed_text, tokenizer)
  prob = model.predict(Prepared_text)
  #prob = [[round(i, 2) for i in prob[0]]]
  #print(prob)
  Pred_class = np.argmax(prob[0])
  labels = {'Functional (F)': 5,
            'Fault Tolerance (FT)': 10,
            'Performance (PE)': 0,
            'Security (SE)': 4,
            'Usability (US)': 2,
            'Availability (A)':3,
            'Legal (L)': 6,
            'Look & Feel (LF)':1,
            'Maintainability (MN)':11,
            'Operational (O)':7,
            'Portability (PO)':8,
            'Scalability (SC)':9
            }
  for key, value in labels.items():
    if value == Pred_class:
      return key

def multiplePrediction(file, model, tokenizer):
    result = {}
    with open(file,'r') as f:
          lines = f.readlines()
    for line in lines[1:20]:
          if len(line)>5:
              result[str(line)] = predict(str(line),model, tokenizer)
    df = pd.DataFrame({'description':result.keys(),'Predicted_class':result.values()})
    file = file[:-3]+'csv'
    df.to_csv(file)
    return file
